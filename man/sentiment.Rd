% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/sentiment.R
\name{sentiment}
\alias{sentiment}
\title{Polarity Score (Sentiment Analysis)}
\usage{
sentiment(text.var, polarity_dt = sentimentr::polarity_table,
  valence_shifters_dt = sentimentr::valence_shifters_table, hyphen = "",
  amplifier.weight = 0.8, n.before = 4, n.after = 2,
  question.weight = 1, but.weight = 0.85, ...)
}
\arguments{
\item{text.var}{The text variable.}

\item{polarity_dt}{A \pkg{data.table} of positive/negative words and
weights with x and y as column names..}

\item{valence_shifters_dt}{A \pkg{data.table} of valence shifters that
can alter a polarized word's meaning and a numic key for negators (1),
amplifiers(2), and de-amplifiers (3) with x and y as column names.}

\item{hyphen}{The character string to replace hyphens with.  Default repalces
with nothing so 'sugar-free' becomes 'sugarfree'.  Setting \code{hyphen = " "}
would result in a space between words (e.g., 'sugar free').}

\item{amplifier.weight}{The weight to apply to amplifiers/de-amplifiers (values
from 0 to 1).  This value will multiply the polarized terms by 1 + this
value.}

\item{n.before}{The number of words to consider as valence shifters before
the polarized word.  To consider the entire beginning portion of a sentence
use \code{n.before = Inf}.}

\item{n.after}{The number of words to consider as valence shifters after
the polarized word.  To consider the entire ending portion of a sentence
use \code{n.after = Inf}.}

\item{question.weight}{The weighting of questions (values from 0 to 1).
Default is 1.  A 0 corresponds with the belief that questions (pure questions)
are not polarized.  A weight may be applied based on the evidence that the
questions function with polarized sentiment.}

\item{but.weight}{The weight to give to "but" style conjunctions that overrule
the previous clause.  Weighting a but statement stems from the belief that
the conjunctions "but", "however", and "although" amplify the current clause
and/or down weight the prior clause.  If a but conjunction is located before
polarized word in the context cluster the cluster is up-weighted 1 + number
of occurrences of the but conjunctions before the polarized word times the
weight given (\eqn{1 + N_{but\,conjunctions} * z_2} where \eqn{z_2} is the
\code{but.weight}).  Conversely,
a but conjunction found after the polarized word in a context cluster
down weights the cluster 1 - number of occurrences of the but conjunctions
after the polarized word times the weight given
(\eqn{1 + N_{but\,conjunctions}*-1 * z_2}).  These are added to the
deamplifier and amplifier weights and thus the down weight is constrained to
-1 as the lower bound.  Set to zero to remove but conjunction weighting.}

\item{\ldots}{Ignored.}
}
\value{
Returns a \pkg{data.table} of:
\itemize{
  \item  element_id - The id number of the original vector passed to \code{sentiment}
  \item  sentence_id - The id number of the sentences within each \code{element_id}
  \item  word_count - Word count
  \item  sentiment - Sentiment/polarity score
}
}
\description{
Approximate the sentiment (polarity) of text by sentence.
}
\details{
The equation used by the algorithm to assign value to polarity of
each sentence fist utilizes the sentiment dictionary (Hu and Liu, 2004) to
tag polarized words.  Each paragraph
(\eqn{p_i = \{s_1, s_2, ..., s_n\}}{p_i = \{s_1, s_2, ... s_n\}}) composed of
sentences, is broken into element sentences
(\eqn{s_i,j = \{w_1, w_2, ..., w_n\}}{s_i,j = \{w_1, w_2, ... w_n\}}) where \eqn{w}
are the words within sentences.  Each sentence (\eqn{s_j}) is broken into a
an ordered bag of words.  Punctuation is removed with the exception of pause
punctuations (commas, colons, semicolons) which are considered a word within
the sentence.  I will denote pause words as \eqn{cw} (comma words) for
convenience.  We can represent these words as an i,j,k notation as
\eqn{w_{i,j,k}}.  For example \eqn{w_{3,2,5}} would be the fifth word of the
second sentence of the third paragraph.  While I use the term paragraph this
merely represent a complete turn of talk.  For example t may be a cell level
response in a questionnaire composed of sentences.

The words in each sentence (\eqn{w_{i,j,k}}) are searched and compared to a
modified version of Hu, M., & Liu, B.'s (2004) dictionary of polarized words.
Positive (\eqn{w_{i,j,k}^{+}}{w_i,j,k^+}) and negative
(\eqn{w_{i,j,k}^{-}}{w_i,j,k^-}) words are tagged with a \eqn{+1} and \eqn{-1}
respectively.  I will denote polarized words as \eqn{pw} for convience. These
will form a polar cluster (\eqn{c_{i,j,l}}{c_i,j,l}) which is a subset of the a
sentence (\eqn{c_{i,j,l} \subseteq s_i,j }{l_i,j,l \subseteq s_i,j}).

The polarized context cluster (\eqn{c_{i,j,l}}) of words is pulled from around
the polarized word (\eqn{pw}) and defaults to 4 words before and two words
after \eqn{pw}) to be considered as valence shifters.  The cluster can be represented as
(\eqn{c_{i,j,l} = \{pw_{i,j,k - nb}, ..., pw_{i,j,k} , ..., pw_{i,j,k - na}\}}{c_i,j,l = \{pw_i,j,k - nb, ..., pw_i,j,k , ..., pw_i,j,k - na\}}),
where \eqn{nb} & \eqn{na} are the parameters \code{n.before} and \code{n.after}
set by the user.  The words in this polarized context cluster are tagged as
neutral (\eqn{w_{i,j,k}^{0}}{w_i,j,k^0}), negator (\eqn{w_{i,j,k}^{n}}{w_i,j,k^n}),
amplifier (\eqn{w_{i,j,k}^{a}}{w_i,j,k^a}), or de-amplifier
(\eqn{w_{i,j,k}^{d}}{w_i,j,k^d}). Neutral words hold no value in the equation but
do affect word count (\eqn{n}).  Each polarized word is then weighted (\eqn{w})
based on the weights from the \code{polarity_dt} argument and then further
weighted by the function and number of the valence shifters directly surrounding the
positive or negative word (\eqn{pw}).  Pause (\eqn{cw}) locations
(punctuation that denotes a pause including commas, colons, and semicolons)
are indexed and considered in calculating the upper and lower bounds in the
polarized context cluster. This is because these marks indicate a change in
thought and words prior are not necessarily connected with words after these
punctuation marks.  The lower bound of the polarized context cluster is
constrained to \eqn{\max \{pw_{i,j,k - nb}, 1, \max \{cw_{i,j,k} < pw_{i,j,k}\}\}} and the upper bound is
constrained to \eqn{\min \{pw_{i,j,k + na}, w_{i,jn}, \min \{cw_{i,j,k} > pw_{i,j,k}\}\}}
where \eqn{w_{i,jn}} is the number of words in the sentence.

The core value in the cluster, the polarized word is acted uppon by valence
shifters. Amplifiers increase the polarity by 1.8 (.8 is the default weight
(\eqn{z})).  Amplifiers (\eqn{w_{i,j,k}^{a}}) become de-amplifiers if the clontext cluster
contains an odd number of negators (\eqn{w_{i,j,k}^{n}}).  De-amplifiers work
to decrease decrease the polarity.  Negation (\eqn{w_{i,j,k}^{n}}) acts on
amplifiers/de-amplifiers as discussed but also flip the sign of the polarized
word.  Negation is determined by raising -1 to the power of the number of
negators (\eqn{w_{i,j,k}^{n}}) + 2.  Simply, this is a result of a belief that two
negatives equal a positive, 3 negatives a negative and so on.

The "but" conjunctions (i.e., 'but', 'however', and 'although') also weight
the context cluster.  A but conjunction before the polarized word
(\eqn{w_{but\,conjunction}, ..., w_{i, j, k}^{p}}) up-weights the cluster by
\eqn{1 + z_2 * \{|w_{but\,conjunction}|, ..., w_{i, j, k}^{p}\}} (.85 is the
default weight (\eqn{z_2})).  A but conjunction after the polarized word
down-weights the cluster by
\eqn{1 + \{w_{i, j, k}^{p}, ..., |w_{but\,conjunction}| * -1\} * z_2}.  The
number of occurrences before and after the polarized word are multiplied by
1 and -1 respectively and then summed within context cluster.  It is this
value that is multiplied by the weight and added to 1. This
corresponds to the belief that a but makes the next clause of greater values
while lowering the value placed on the prior clause.

The researcher may provide a weight \eqn{z} to be utilized with
amplifiers/de-amplifiers (default is .8; de-amplifier weight is constrained
to -1 lower bound).  Last, these weighted context clusters (\eqn{c_{i,j,l}}{c_i,j,l}) are
summed (\eqn{c'_{i,j}}{c'_i,j}) and divided by the square root of the word count (\eqn{\sqrt{w_{i,jn}}}{\sqrtn w_i,jn}) yielding an unbounded
polarity score (\eqn{\delta}{C}) for each sentence.

\deqn{\delta=\frac{c'_{i,j}}{\sqrt{w_{i,jn}}}}{C=c'_i,j,l/\sqrt(w_i,jn)}

Where:

\deqn{c'_{i,j}=\sum{((1 + w_{amp} + w_{deamp})\cdot w_{i,j,k}^{p}(-1)^{2 + w_{neg}})}}

\deqn{w_{amp}= (w_{b} > 1) + \sum{(w_{neg}\cdot (z \cdot w_{i,j,k}^{a}))}}

\deqn{w_{deamp} = \max(w_{deamp'}, -1)}

\deqn{w_{deamp'}= (w_{b} < 1) + \sum{(z(- w_{neg}\cdot w_{i,j,k}^{a} + w_{i,j,k}^{d}))}}

\deqn{w_{b} = 1 + z_2 * w_{b'}}

\deqn{w_{b'} = \sum{\\(|w_{but\,conjunction}|, ..., w_{i, j, k}^{p}, w_{i, j, k}^{p}, ..., |w_{but\,conjunction}| * -1}\\)}

\deqn{w_{neg}= \left(\sum{w_{i,j,k}^{n}}\right) \bmod {2}}
}
\note{
The polarity score is dependent upon the polarity dictionary used.
This function defaults to the word polarity dictionary used by Hu, M., &
Liu, B. (2004), however, this may not be appropriate for the context of
children in a classroom.  The user may (is encouraged) to provide/augment the
dictionary (see the \code{sentiment_frame} function).  For instance the word
"sick" in a high school setting may mean that something is good, whereas
"sick" used by a typical adult indicates something is not right or negative
connotation (\strong{deixis}).
}
\examples{
mytext <- c(
   'do you like it?  But I hate really bad dogs',
   'I am the best friend.',
   'Do you really like it?  I\\'m not a fan'
)
sentiment(mytext)
sentiment(mytext, question.weight = 0)

sentiment(gsub("Sam-I-am", "Sam I am", sam_i_am))
plot(sentiment(gsub("Sam-I-am", "Sam I am", sam_i_am)))

y <- "He was not the sort of man that one would describe as especially handsome."
sentiment(y)
sentiment(y, n.before=Inf)
}
\references{
Hu, M., & Liu, B. (2004). Mining opinion features in customer
reviews. National Conference on Artificial Intelligence.

\url{http://www.slideshare.net/jeffreybreen/r-by-example-mining-twitter-for}

\url{http://hedonometer.org/papers.html} Links to papers on hedonometrics
}
\seealso{
\url{https://github.com/trestletech/Sermon-Sentiment-Analysis}

Other sentiment.functions: \code{\link{sentiment_by}}
}
\keyword{polarity}
\keyword{sentiment,}

